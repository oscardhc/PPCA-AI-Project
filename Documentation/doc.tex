\documentclass[UTF8,a4paper，12pt]{article}
\usepackage[UTF8]{ctex}

%% compile with xelatex
%\usepackage{mathpazo}           % set math font
%\usepackage{fontspec}  % to load non-Latex fonts (keeping math font)
%\setmainfont{Palatino}
%\setsansfont{Optima}
%\usepackage{sectsty}
%\allsectionsfont{\fontspec{Optima}}  % set header font
%
%\newfontfamily\menlo{Menlo}

\usepackage{listings-ext}
\lstset{language=Python,basicstyle=\footnotesize\menlo,}
\usepackage{xunicode, xltxtra, listings, geometry, indentfirst, xeCJK, amsmath, clrscode, enumerate, indentfirst, wrapfig, color, caption, amssymb, multicol, ulem, esvect, bm, amsthm, extarrows, setspace}
\usepackage[colorlinks,linkcolor=blue]{hyperref}

\linespread{1.5}\selectfont


%\usepackage{eulervm}

\setlength{\parindent}{2em}

\newcounter{RomanNumber}
\newcommand{\mrm}[1]{(\setcounter{RomanNumber}{#1}\Roman{RomanNumber})}

%\geometry{top=3cm,bottom=3cm}

\def \e {\mathrm{e}}
\def \dd {\mathrm{d}}
\newcommand{\vc}[1]{\overrightarrow{#1}}
\newcommand{\sq}[1]{\sqrt{#1}}
\newcommand{\fr}[2]{\dfrac{#1}{#2}}
\def\st {\mathrm{s.t.}}
\def\eps {\varepsilon}
\def\ds {\displaystyle}
\def\CC {\mathbb{C}}
\def\FF {\mathbb{F}}
\def\GG {\mathbb{G}}
\def\QQ {\mathbb{Q}}
\def\RR {\mathbb{R}}
\def\NN {\mathbb{N}}
\def\ZZ {\mathbb{Z}}
\def\ii {\mathrm{i}}
\def\rtrans {\overset{r}{\rightarrow}}
\def\ctrans {\underset{c}{\rightarrow}}
\def\R {\mathrm{rank}}
\def\cd {\cdot}
\def\TR {\mathrm{trace}}
\def\DET {\mathrm{det}}
\def\LRA{\Leftrightarrow}
\def\dsum {\ds\sum}
\newcommand{\eq}[1]{\xlongequal{\mathrm{#1}}}
\def\smtn{\ds\sum_{i=1}^n}
\def\End{\mathrm{End}}
\def\Hom{\mathrm{Hom}}
\def\im{\mathrm{Im}}
\def\ker{\mathrm{Ker}}
\newcommand{\sm}[3]{\ds\sum_{#1=#2}^{#3}}
\newcommand{\mttt}[4]{
\left[ \begin{matrix}
		#1 & #2 \\
		#3 & #4
	\end{matrix} \right]
}
\newcommand{\solpic}[2]{\begin{center}
\includegraphics[ height=#1\baselineskip ]{#2.pdf}
\end{center}}
\def\MC {\mathcal}

\theoremstyle{theorem}
\newtheorem{thm}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{定义}
\newtheorem{jielun}{结论}

\newcommand{\qu}[1]{\quu{#1}\vspace{0cm}}

\title{Image-to-image translation Documentation}
\author{518030910417 董海辰 \\ 518030910440 庄永昊}
\date{2019年8月}

\begin{document}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	
	\section{我们要做什么？}
		\subsection{I2I(image to image)}
		
广义上，I2I(image to image)泛指所有满足以下要求的任务：输入为一个图像，输出为符合要求的另一个图像。在两个图像中，有部分信息保持不变，正如无论是用“苹果”还是“apple”来表示，所指代的对象都是相同的事物。当前的I2I任务，主要包括风格迁移、图像降噪、超分辨率、语义分割等。本次实验中我们关注的是图像风格迁移及其中间过程。一个典型的图像风格迁移的案例，是将图片中的马识别出来并转换为斑马，或反向进行相同的转换。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{assets/01}
	\caption{\href{https://arxiv.org/abs/1703.10593}{风格迁移, 2017}}
\end{figure}

		\subsection{一些基础的知识}
\subsubsection{插值算法}

既然是从一个状态到另一个状态，一个显然的思路是量化两个状态，再通过数值的中间值反向得到中间状态。获得数值中间值的方法就是插值。简单讲，插值就是根据已知数据点（条件），来预测未知数据点值的方法。最简单的插值就是线性插值，即根据状态$x_1, x_2$和一个表示中间值相对位置的值$\eps ( \eps \in [0, 1])$得到中间值$\eps x_1+(1-\eps)x_2$。除此之外，常用的插值还有双线性插值、Slerp(spherical linear interpolation)等方法。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{assets/02}
	\caption{\href{https://devblogs.nvidia.com/photo-editing-generative-adversarial-networks-1}{常用插值方法，在两个黑色点之间作插值}}
\end{figure}

\subsubsection{同态}

与插值算法相对应的一个概念是隐空间(latent space)。正如前文所述，在状态过渡时，我们需要先量化两个状态，再进行插值。任意一个状态$\Sigma$都能被量化为一个对应向量$t$，这些向量$t$的全体就分布在对应的隐空间中。状态间的过渡也可以用以下步骤进行描述：
\begin{enumerate}
	\item 从起始状态映射至隐空间；
	\item 在隐空间中进行过渡（插值）；
	\item 从隐空间映射至最终状态。
\end{enumerate}

显然，状态与隐空间中有意义的点之间是一一对应的关系，在拓扑上，这种关系也被称为同态。

\subsubsection{卷积}

如果直接使用插值，常常出现我们不想看到的一种情况：如果将一个人的脸转换为另一个人，在图像里两张人脸有略微的不对齐时，中间过程里会出现若隐若现的四个眼睛！这是由于我们的插值算法无法辨别出两张图片里五官的位置。显然，我们希望我们的程序能“聪明”地识别出图像中内容的特征信息。

这些特征信息有什么特点呢？首先，像五官这样的信息在图片里往往是连续的像素点，此外，眼睛、头发等特征的颜色往往与皮肤的颜色有很大区别。因此，我们可以将图片中相邻一片像素点的颜色值（RGB值）作为一个可能的特征。卷积正是进行了这样的操作。在卷积过程中，我们取一个较小的矩阵，例如3×3，然后赋予这个矩阵每个点一个值。随后，我们将这个矩阵叠在图片上，将每一对重叠的数字相乘以后求和，就得到了卷积的值。而上述的矩阵，被我们称为卷积核。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{assets/03}
	\caption{\href{http://ex2tron.top}{卷积}}
\end{figure}


卷积可以提取出抽象的特征，通常，我们认为一次或连续多次优秀的卷积，能够将输入的内容映射至前文所述的，包含所有可以表示特征的向量的隐空间里。

\subsubsection{神经网络}

在计算机领域，神经网络往往是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应。

神经网络中最基本的成分是神经元模型，即上述的“简单单元”。在神经网络中，每个神经元与其它神经元相连，当它“兴奋”时，就会向相连的神经元发送信号；当某个神经元接受到的信号超过一个“阈值”时，它就会被激活，即“兴奋”起来。这一模型也被称为“M-P神经元模型”。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/04}
	\caption{M-P神经元模型，周志华《机器学习》}
\end{figure}


理想的激活函数应当形如阶跃函数，即在输入大于某个值，神经元完全兴奋，否则完全不兴奋。然而，由于阶跃函数有不连续、不光滑等性质，因此实际常使用一些连续的，形状与阶跃函数相似的函数作为激活函数，如Sigmoid函数。由于它将较大范围内变化的输入值挤压到$[0,1]$输出值范围内，因此有时也称其为“挤压函数”。常用的激活函数还包括ReLU函数及其衍生等。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/05}
	\caption{激活函数，左为阶跃函数，右为Sigmoid函数}
\end{figure}

将许多个这样的神经元按照一定的层次结构连接起来，就得到了神经网络。若神经元分属不同层次，每一层都只接收前一层的信号，并将信号传递至后一层，则称其为前馈神经网络。前馈神经网络有许多优秀的模型，通常我们说“神经网络”时，指代的就是前馈神经网络。前馈神经网络中，在输入层与输出层之间的神经元层次，被称为隐层或隐含层。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{assets/06}
	\caption{\href{https://nndl.github.io/nndl-book.pdf}{多层前馈神经网络}}
\end{figure}

根据Hornik和Cybenko提出的万能近似定理，一个前馈神经网络中，如果具有线性输出层和至少一层具有“挤压”性质的隐层，则只要这个神经网络有足够多的隐藏单元，就能以任意精度来近似任意一个从有限维空间到有限维空间的Borel可测函数。因此，从理论上，神经网络可以很好地拟合得到几乎所有目标函数。

\subsubsection{误差逆传播与梯度下降}

如何对神经网络中繁多的参数进行更新呢？目前常用的方法被称为误差逆传播算法，常用于多层前馈神经网络。一个简单的神经网络如上图所示，包含输入层、隐藏层和输出层。假设输入层共有d个结点，用$x_1,x_2...x_d$，隐层上共有q个结点，编号为$b_1,b_2...b_q$，其阈值为$\gamma_1,\gamma_2...\gamma_q$。输出层共有l个结点，依次为$y_1,y_2...y_l$，其阈值为$\theta_1,\theta_2...\theta_l$。不失一般性，假设每一层都是全连接层，激活函数为Sigmoid函数。

为了方便书写，假设输入层至隐层的连接权为$v_{ij}, i=1,2...d j=1,2...q$，隐层至输出层的连接权为$w_{ij}, i=1,2...q j=1,2...l$，且假设隐层的输入为$\alpha_i i=1,2,...q$，输出层的输入为$\beta_i i=1,2,...l$。

对于一个训练例$(x_{k},y_{k})$，神经网络的输出为$\hat{y_{k}}=(\hat{y_1^k},\hat{y_2^k}...\hat{y_l^k})}$。则网络的均方误差为

$$\mathrm E_{k}=\frac{1}{2}\sum_{j=1}^{l}{\hat{(y_j^k}-y_j^k)^2}$$

任意参数$v$的更新估计式为：
$v \leftarrow v + \Delta v$

下面以隐层到输出层的连接权$w_{hj}$为例描述这一算法：

BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。对于一个给定的学习率$\eta$（用于衡量每次梯度下降的程度，在0至1之间），有

$$\Delta w_{hj}=-\eta\frac{\partial\mathrm E_{k}}{\partial w_{hj}}$$

注意到$w_{hj}$先影响道第$j$个输出层神经元的输入值$\beta_{j}$，再影响到其输出值$\hat{y_{j}^{k}}$，然后影响到$\mathrm E_{k}$，有

$\frac{\partial\mathrm E_{k}}{\partial w_{hj}}=\frac{\partial\mathrm E_k}{\partial\hat{y_{j}^{k}}}\cdot\frac{\partial\hat{y_{j}^{k}}}{\partial\beta_{j}}\cdot\frac{\partial\beta_{j}}{\partial w_{hj}}$

由定义有$\frac{\partial\beta_{j}}{\partial w_{hj}}=b_{h}$

Sigmoid函数在任意位置的导数均存在且可以被解出（其余连续的激活函数均满足这一要求）。有

$$g_{j}=-\frac{\partial\mathrm E_{k}}{\partial\hat{y_{j}^{k}}}\cdot\frac{\partial\hat{y_{j}^{k}}}{\partial\beta_{j}}=-(\hat{y_{j}^{k}}-y_{j}^{k})\cdot f'(\beta_{j}-\theta_{j})$$

则$w_{hj}$的更新公式为$$\Delta w_{hj} = \eta g_jb_h$$

\subsubsection{GAN}

GAN（生成式对抗网络）当中，两个神经网络相互竞争，生成网络从隐空间中随机取样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。进一步细分，生成网络还可以分为三部分：提取特征（Encoder），中间过程和翻译特征（Decoder）。

\section{最初的尝试：直接使用插值}

当图片完全对齐时，使用线性插值的结果如下（最左、最右侧的图片分别是起始、目标图片，下同）：
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/07}
	\includegraphics[width=0.8\textwidth]{assets/08}
	\caption{线性插值}
\end{figure}

看起来还不错，可这是由于我们的初始、目标图片的五官位置几乎完全一致。一旦图片里人的五官有不对齐，就会变成这样：

中间过程里，生成的人脸甚至出现了四只模糊的眼睛！显然，这个方法是不够普适的，我们无法保证所有目标图像的重要信息都被存储在同一块区域里，它们可能出现在相似却不完全重合的位置。因此，我们此前介绍的卷积就该发挥作用了。

\section{使用神经网络：CycleGAN}

\subsection{为什么使用CycleGAN？}

正如上文所述，传统的插值算法无法得到图片中具体内容的特征。为此，我们可以引入卷积网络。为了得到较为优秀的卷积核的值，可以使用神经网络进行训练。

既然我们的中间图片是否优秀依赖于图片具体内容的特征，假设我们已经知道了我们的起始图片和目标图片的属性（为了避免混淆，我们将卷积提取到的内容称为特征，将具有具体意义的，例如眼睛、头发颜色等，称为属性），如果属性足够丰富，那么我们只要将我们起始属性与目标属性不同的那部分改变为目标属性，就实现了图片的转换与部分中间过程。例如，将一个在笑的男人变为一个没有笑的女人，我们可以先生成一个在笑的女人，再将这张生成的照片进一步转换为没有笑的女人即可。

现在，我们的目标变为将具有某一属性（如“在笑”）的照片，转换为不具有该属性的照片，同时保证其余内容尽可能不改变。而这正是CycleGAN所擅长的。

\subsection{CycleGAN是什么？}

CycleGAN用于解决在两个域之间相互转换的问题，正如其名字所示，它的结构是一对对偶的GAN。

假设我们想要将域A与域B上的内容相互转换（例如在上文中，域A是“在笑的人”，而域B是“不在笑的人”），那么将会有一个生成网络负责将域A上的内容变为域B上的内容，一个判别网络负责鉴定当前内容是否在域B上，抑或是生成网络制造的错误图片。对称地，还有一个从B至A的生成网络和判别是否是A上天然图片的判别网络。

为了让这两个GAN可以相互帮助对方进行修正，CycleGAN引入了重构误差，也就是将域A上的内容通过生成网络映射至域B后，再通过另一个生成网络重新映射回A，然后再利用对应的判别网络进行鉴别，并修改两个生成网络的参数以尽量迷惑鉴别网络。对于B上的内容，有一套对称的步骤。

\subsection{一些额外的小技巧}

神经网络的结构采用了ResNet模型。ResNet模型最初被用于解决神经网络层数加深至一定数量，性能降低、计算速度不足的问题。在ResNet模型中，较深层的输入由两部分构成，其一为上一层的输出信息（被称为残差），其二为多层以前的输出信息（称为恒等项），如下图所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/09}
	\caption{ResNet模型。$\MC F(\mathrm x)$是残差项，而$\mathrm x$是恒等项}
\end{figure}

实际上，这相当于将学习的目标从被提取的特征本身，变为了被提取特征与输入值的差，正因此，输出信息才会被称为“残差”。在进行反向梯度传播时，这一网络只有在极少情况下才会造成残差项与恒等项的梯度相同（恒等项梯度仅由激活函数决定，与一个常数项类似），反向传播梯度消失的情况。因此理论上，如果网络已经到达最优，继续加深网络，残差项的影响将会变为0，仅剩恒等项。即深度过高时，网络的结构相当于保持不变，仍然处于最优状态。

除了ResNet以外，还有一个常用模型，这就是U-Net模型。该模型结构如下所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/10}
	\caption{U-net模型，前半部分被称为下采样，后半部分被称为上采样}
\end{figure}

U-net模型中的网络是对称的结构，将第i层的输出加入到倒数第i层的输入中。这一模型融合了不同尺度的特征，同时能保证上采样恢复的特征不会过于粗糙。

\subsection{结果如何？}

我们取三组属性：Young, Blond Hair, Male进行训练，以128为batch size各进行了30000次迭代，得到了以下结果。

下图中，人像由“非年轻黑发男性”渐变为了“年轻金发女性”。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.2\textwidth]{assets/14-0}
	\includegraphics[width=0.2\textwidth]{assets/14-1}
	\includegraphics[width=0.2\textwidth]{assets/14-2}
	\includegraphics[width=0.2\textwidth]{assets/14-3}
	\caption{}
\end{figure}

\subsection{CycleGAN的问题}

首先，这样的转换依赖于图片中的内容的属性，因此需要手动对图片的属性进行标注。当然，我们也可以手动训练一个网络来对图片中的属性进行标注。

此外，正如结果所示，在图片属性不够丰富时，我们转换的最终结果与我们期望的结果并不相似。然而，当图片属性过多，训练这些网络的开销又会变得非常庞大，且图片的转换过程变得非常漫长。

最后一个问题是，我们其实并不能获得所有有意义的中间状态。例如，当我们将一个在大笑的男人转换为不在笑的女人时，在“在大笑的女人 $\rightarrow$ 不在笑的女人”这一过程里，我们不能获得位于其间的“在微笑的女人”。

\section{新的思路：引入一个同态}

\subsection{为什么需要同态？}

将生成网络细分为三个部分：提取特征（Encoder）、中间过程和翻译特征（Decoder）。只要我们能在提取特征后数据所在的隐空间里找到一个有效、连续的插值，上面的CycleGAN的问题就得到了解决。

在实践中我们发现，如果在隐空间上作均匀的线性变化，对应图像上特定属性的变化并不均匀。相反，它往往在某一段区间上突变。如下图所示，我们希望将人脸的笑容隐去，并控制其间笑容逐渐淡化的程度（如第二行所示），但此前的方法并不能完成我们的目标（如第一行所示），图片在c与d之间有较大的变化，却在其余部分几乎不变。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{assets/11}
	\caption{}
\end{figure}
因此，在隐空间上进行插值时，线性函数效果不佳，需要一个非线性的函数。同时，CycleGAN中低效的问题仍然无法解决。

综上所述，我们的新算法应当让卷积得到的特征尽可能表示图像中的所有属性，并且使用神经网络来拟合一个个插值。尽管隐空间上的插值无法使用线性插值，但现实属性的变换与过渡，是可以使用线性插值来完成的。如果我们能让现实属性的过渡与隐空间上的插值形成同态的关系，就可以通过现实属性的过渡来纠正隐空间上神经网络拟合插值函数的误差。

\subsection{如何实现同态？}

在神经网络中，生成器被分为三个部分：提取特征（Encoder）、插值、翻译特征（Decoder）。其中Encoder与Decoder的过程，相当于实现图片与隐空间之间的相互映射。插值部分包括了多个并列的神经网络，每一个网络对应了一个有现实意义的属性。插值的最终结果等于每个神经网络结果的加和。由于插值结果与现实属性是同态的，而现实中属性的过渡可以用线性的变换，故可以直接对插值结果均匀变换，来调整某个属性的变化程度。

为了校正用于插值的网络，需要额外引入一个网络$\MC A'(\cdot)$ ，其作用是拟合得到一个函数，可以将插值结果映射至现实属性，从而得到对应的误差。

总的来说，我们的模型的结构如下：

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{assets/mdl}
	\caption{}
\end{figure}

训练时，读入两组图片$\mathrm Im_a$、$\mathrm Im_b$和它们自身的一些属性$\mathrm z_a$、$\mathrm z_b$。图片首先通过Encoder网络$\MC E$，得到对应的特征矩阵(将起始图片和目标图片的特征分别记为$\mathrm f_a$和$\mathrm f_b$)。

接下来我们将两个特征矩阵通过插值器$\MC I$进行插值，得到$\mathrm F_{interp}$。插值器是一组结构相同的神经网络，插值的方法如下：

$$\mathrm F_{interp} = \mathrm F_a + \sum_{i=1}^{c}{v_i \cdot \MC T_i(\mathrm F_b - \mathrm F_a)}$$

其中，$\MC T_i$代表第$\mathrm i$个属性的插值函数，是一个简单的神经网络，$v_i$代表了第$\mathrm i$个属性的变化程度，在0和1之间，越靠近0，表示这一属性的变化程度越小。这一插值的原理还会在下文中插值部分进一步解释。

在插值后，我们将得到的特征$\mathrm F_{interp}$通过另一个神经网络$\MC D$来实现翻译特征（Decoder）的过程，以得到最终的图片。

为了对每一个神经网络的参数进行更新，还额外引入了一些训练中的过程：

首先，生成的内容需要通过一个神经网络$\MC Dis$进行鉴别，我们采用了WGAN，但鉴别的对象并不是生成的图片，而是插值得到的特征$\hat{\mathrm F}$。（这一特征会与图片中直接提取出的特征$\mathrm F$进行比较）为了让训练更稳定，还使用了梯度惩罚。

$$\min_{\MC Dis} \MC L_{\mathrm GAN_{\MC Dis}} = \mathbb E_{\mathbb P_{interp}}[\MC Dis(\hat{\mathrm F})] - \mathbb E_{\mathbb P_{real}}[\MC Dis(\mathrm F)] + \lambda_{gp}\MC L_{gp}    (4.1)$$

$$\min_{\MC E,I} \MC L_{\mathrm GAN_{\MC E,I}} = \mathbb E_{\mathbb P_{real}}[\MC Dis(\mathrm F)] - \mathbb E_{\mathbb P_{interp}}[\MC Dis(\hat{\mathrm F})]    (4.2)$$

神经网络$\MC Dis$的参数也通过这一距离来更新，体现了网络中对抗的成分。

对于生成器中Encoder和Decoder部分$\MC E$、$\MC D$，额外增加一个步骤来更新其参数：将Encoder的结果不通过插值器，直接输入Decoder生成图片。得到的图片与原图的差距即可被用于更新这两个网络的参数。实践表明，计算距离时直接将两张图片各像素点的RGB值相减的效果较差，因此，我们引入一个已训练好的、在判别图片差距这一模型上表现优秀的网络VGG来计算两张图片的差距：

$$\min_{\MC D} \MC L_{\MC D} = \mathbb E(||\Phi_3(\MC D(\mathrm F)) - \Phi_3(\MC Im)||^2)    (4.3)$$

$$\min_{\MC E} \MC L_{recon} = \mathbb E(||\Phi_3(\MC D(\mathrm F)) - \Phi_3(\MC Im)||^2)    (4.4)$$

VGG网络给出图片之间差距的方法实质上是通过卷积提取各个局部的特征，从而对图片的每个小块的距离进行比较。这一原理与$\MC E$的目标很相似。因此，我们可以用VGG中更深层的部分来更新$\MC E$的参数：

$$\min_{\MC E,I} \MC L_{\mathrm KG} = \mathbb E_{\mathbb P_{real}}||\MC P[\MC E(\mathrm Im)] - \Phi_5(\mathrm Im)||^2    (4.5)$$

其中，网络$\MC P$只含有一个卷积核大小为$1*1$的卷积层，目的仅在于让两个被比较对象的形状相同。

现在，除了插值以外的两个部分的参数可以得到充分的更新了。$\MC D$的更新如公式4.3所示，$\MC E$的更新通过以下公式：

$$\MC L_{\MC E} = \lambda_{\mathrm GAN_{\MC E}}\MC L_{\mathrm GAN_{\MC E,I}} + \lambda_{recon}\MC L_{recon} + \lambda_{\mathrm KG}L_{\mathrm KG}    (4.6)$$

过程中引入的网络$\MC Dis$、$\MC P$的更新分别通过公式4.1与4.5所示。

现在我们来看插值的部分。正如上文所述，我们引入了一个网络$\MC A'(\cdot)$，模拟从插值得到的矩阵$\MC T(\mathrm F_b - \mathrm F_a)$到属性$\mathrm z$（与图片一同读入，随插值改变）的同态。注意$\MC T$实际上是一组网络$\MC T_1...T_c$。对两张图片的属性，可以直接使用线性插值，即

$$\mathrm z_{interp} = \mathrm z_a + \sum_{i=1}^{c}{v_i \cdot (\mathrm z_b - \mathrm z_a)}$$

上文中，两个特征的差通过插值器后得到的矩阵可以直接与代表插值程度的系数相乘，正是由于插值器得到矩阵与属性的同态，和属性可以进行上述线性插值。通过这一同态，我们可以更新插值网络的参数了：

$$\min_ {\MC I} \MC L_{\mathrm F_{hom}} = \mathbb E [-\mathrm z_{interp}\log(\MC A'(\mathrm F_{interp}))]    (4.7)$$

网络$\MC A'(\cdot)$的误差（被称为同态差距）也可以通过类似的方式更新：

$$\min_ {\MC A'(\cdot)} \MC L_{\MC A'(\cdot)} = \mathbb E [-\mathrm z_a\log(\MC A'(\mathrm F_{interp}))]    (4.8)$$

尽管将$\mathrm z_a$与$\MC A'(\mathrm F_a)$比较更符合逻辑，但实际实践中，这会让插值器效果变差，因此其意义被改为让插值器尽量减少变化的内容，从而让修改特定属性的插值器不改变其余内容。

此外，还可以将插值强度设为最高，即令插值器将$\mathrm F_a$彻底转换为$\mathrm F_b$，然后用插值结果与$\mathrm F_b$比较，也可以用于更新插值器:

$$\MC L_{\MC I_{total}} = ||\mathrm F_{interp,v=1} - \mathrm F_b||^2$$

综上所述，插值过程的参数更新为：

$$\MC L_{\MC I} = \lambda_{\mathrm GAN_{\MC I}}\MC L_{\mathrm GAN_{\MC E,I}} + \lambda_{\MC I_{hom}}\MC L_{\mathrm F_{hom}} + \lambda_{\MC I_{total}}\MC L_{\MC I_{total}}    (4.9)$$

过程中引入的网络$\MC A'(\cdot)$的更新方法如公式4.8所示。

\subsection{其它技巧}

注意到训练的主要目标是获得一个较好的插值函数，因此我们的判别器被用于判定插值的结果，而非这个结果输入Decoder后输出的图片。而$\MC A'(\cdot)$同样是用于计算插值后结果与目标属性之间误差的交叉熵。因此，这两个网络具有一定的相似性。为了加快模型的训练速度，我们令$\MC A'(\cdot)$与$\MC Dis$的前几层共用，只在最后一层有所区别。

\subsection{实现细节}

我们在论文的基础上，新增加了一些分组，以下9组条件更为全面描述了人脸的特征：

\begin{tabular}{c|c}
  编号 & 分组 \\
  \hline
  1 & Mouth Slightly Open, Smiling \\
  2 & Male,No Beard,Mustache,Goatee,Sideburns \\
 3 & Black Hair,Blond Hair,Brown Hair,Gray Hair\\
 4 & Bald,Receding Hairline,Bangs\\
 5 & Young\\
 6 & Arched Eyebrows,Bags Under Eyes,Bushy Eyebrows,Eyeglasses\\
 7 & Big Lips,Big Nose,Chubby,Double Chin,High Cheekbones,Narrow Eyes,Pointy Nose\\
 8 & Straight Hair,Wavy Hair\\
 9 & Attractive,Pale Skin,Heavy Makeup
\end{tabular}

训练过程中，我们采用Adam Optimizer，其中$lr = 0.0002, betas = \{0.5, 0.999\}$。

使用了"CelebA"数据集，含有192000张带有标记的人像图片。以48为batch size，128x128为image size，进行了共128000次迭代。

\subsection{最终成果}

下图展示了模型的最终成果，从左侧图片到右侧图片实现了人脸的转换，其中转换的对象依次为表情、性别、头发颜色、头发形状、年龄和其它信息，其中左起第一列图片是原图，第二列是不经过插值后直接重建（encoder-decoder）的图片，第三列起每两列是一种属性的转换，转换强度分别为0.5、1。最后一列是目标图像。

\newpage
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{assets/11}
	\caption{}
\end{figure}

下图则更细致地对单一属性的转换给出了结果：（所选属性为表情）

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1.0\textwidth]{assets/12}
	\caption{}
\end{figure}

\subsection{问题与改进}

在训练过程中，我们向测试集中加入了一些额外的图片，发现在训练相同时间时的效果远弱于数据集中自带的测试集。我们认为这是由于数据集中的头像位置几乎重合，因此我们加入的图片即使稍有位置的差异，也会造成效果不佳的问题。于是我们在训练过程中，手动为图像增加了位置的变化等干扰。

此外，人脸及其表情本身具有较好的连续性，但很多数据集并没有这一优势。因此，当我们将这一模型应用于一些图像变化连续性弱的领域时，得到的结果并不令人满意，这是这一网络没有考虑到的问题。

最终训练得到的模型中，输入图片背景为白色时，输出图片背景会变灰。我们认为这可能是由于网络的层次过深，导致Encoder与Decoder中每个点都与整张图片中大部分都相关，从而让原本为白色的部分也受到了其余位置的颜色的影响。

这个问题的另一个思路是额外增加一个attribute，称其为other attributes。它只在插值过程中发挥作用，并不在计算loss与验证时发挥作用，且起始图片的值为0，目标图片的值为1。

在最初训练的结果中，我们的效果很差。因此，我们对原文的方法进行了改进：训练过程中，通过decoder生成的图片会通过另一个discriminator，以patchGAN的方法（即在判断图片是否为真时，将图片分割为若干区域，每个区域单独通过判别器网络，最后得到一个较小的张量，表示各个区域是否为真）判断该图是否为真实图片并进行更新。此外，我们修改了学习率衰减的参数（减缓了衰减），终于得到了上图所示的较好的结果。

\end{document}